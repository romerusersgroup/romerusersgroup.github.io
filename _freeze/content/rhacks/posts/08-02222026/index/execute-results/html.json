{
  "hash": "e6371f1f2484b607f226328295ead743",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"CSV vs Parquet: A Better Default for Analytics\"\nsubtitle: \"Faster reads, smaller files, cleaner workflows\"\ndescription: \"R-Hacks N.8\"\nauthor: \"Federica Gazzelloni\"\ndate: \"22 February 2026\"\nimages: \"images/image-1.png\"\nseries: R-Hacks\ncategories:\n  - Parquet\n  - Arrow\n  - data-storage\n  - performance\nlayout: single\ntoc-location: left\ntoc: true\ncode-overflow: wrap\nexecute:\n  warning: false\n  message: false\n---\n\n<br>\n\n![CSV vs Parquet: A Better Default for Analytics (ChatGPT generated image)](images/image-1.png){width=\"80%\" fig-align=\"center\" fig-alt=\"ChatGPT generated image\"}\n\n\nIf you are still saving analytical datasets as .csv by default, you are likely paying a hidden tax:\n\n- slower reads\n- larger files\n- fragile type handling\n- unnecessary reprocessing\n\nCSV is universal.\nBut universal does not mean optimal.\n\nThis R-Hack explains why Parquet should often be your default for analytical workflows.\n\n\n## What Is Parquet?\n\nParquet is a columnar, binary storage format designed for analytics.\n\nUnlike CSV:\n\n- It stores column types explicitly\n- It compresses data efficiently\n- It reads columns independently\n\nIn R, Parquet support is provided by the arrow package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example R code chunk\n# Load necessary libraries\nlibrary(arrow)\nlibrary(dplyr)\n\n# Create a sample data frame\ndata <- data.frame(\n  id = 1:5,\n  value = c(10.5, 20.3, 30.8, 40.2, 50.1)\n)\n\n# Write the data frame to a Parquet file\nwrite_parquet(data, \"sample_data.parquet\")\n\n# Read the Parquet file back into R\ndata_read <- read_parquet(\"sample_data.parquet\")\n\n# Print the data\nprint(data_read)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 √ó 2\n     id value\n  <int> <dbl>\n1     1  10.5\n2     2  20.3\n3     3  30.8\n4     4  40.2\n5     5  50.1\n```\n\n\n:::\n:::\n\n\n\n## Step 1 ‚Äî Simulate a Realistic Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ndf <- data.frame(\n  id = 1:200000,\n  category = sample(LETTERS[1:5], 200000, replace = TRUE),\n  value = rnorm(200000),\n  flag = sample(c(TRUE, FALSE), 200000, replace = TRUE)\n)\n```\n:::\n\n\nThis mimics a moderately sized analytical dataset.\n\n## Step 2 ‚Äî Save and Read as CSV\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(df, \"data.csv\", row.names = FALSE)\n\nsystem.time({\n  df_csv <- read.csv(\"data.csv\")\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.255   0.011   0.268 \n```\n\n\n:::\n:::\n\n\nNow check file size:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.info(\"data.csv\")$size\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6819327\n```\n\n\n:::\n:::\n\n\nCSV:\n\n- stores everything as text\n- requires parsing every time\n- guesses types on read\n\n\n\n## Step 3 ‚Äî Save and Read as Parquet\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_parquet(df, \"data.parquet\")\n\nsystem.time({\n  df_parquet <- read_parquet(\"data.parquet\")\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.008   0.001   0.007 \n```\n\n\n:::\n:::\n\n\nCheck file size:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.info(\"data.parquet\")$size\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3231687\n```\n\n\n:::\n:::\n\n\n\nYou will typically observe:\n\n- faster read time\n- smaller file size\n- preserved data types\n\n\n> **Why This Matters in Practice?**\n\n### 1Ô∏è‚É£ Speed\n\nParquet reads are faster because:\n\n- it stores columns independently\n- it avoids text parsing\n- it uses compression effectively\n\nThis becomes significant as datasets grow.\n\n\n### 2Ô∏è‚É£ File Size\n\nParquet compresses automatically. In many real-world cases parquet files are 30‚Äì70% smaller than CSV equivalents:\n\n- Less storage\n- Faster transfer\n- Cleaner repositories\n\n\n### 3Ô∏è‚É£ Type Safety\n\nCSV does not store types.\n\nEvery read operation reinterprets:\n\n- logical values\n- factors\n- dates\n- numeric columns\n\nParquet preserves them.\n\nThis reduces silent coercion bugs.\n\n## Bonus ‚Äî Select Columns Without Loading Everything\n\nWith arrow, you can load only what you need:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_parquet(\"data.parquet\", col_select = c(\"id\", \"value\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 200,000 √ó 2\n      id   value\n   <int>   <dbl>\n 1     1  0.945 \n 2     2 -0.284 \n 3     3  0.395 \n 4     4  1.32  \n 5     5  0.872 \n 6     6  0.0354\n 7     7 -0.427 \n 8     8 -0.975 \n 9     9  0.0965\n10    10  0.808 \n# ‚Ñπ 199,990 more rows\n```\n\n\n:::\n:::\n\n\n\nThis is especially powerful for:\n\n- large analytical datasets\n- remote storage\n- modular pipelines\n\n### When CSV Is Still Appropriate\n\nCSV is fine when:\n\n- exporting for non-technical users\n- sharing small files\n- generating human-readable outputs\n\n> **But for analytical workflows?**\n\nParquet is usually superior.\n\n\n## Workflow Recommendation\n\nFor reproducible projects:\n\n- Raw data ‚Üí store as Parquet\n- Intermediate processed data ‚Üí store as Parquet\n- Final exports ‚Üí optionally CSV\n\nThink of Parquet as your working format,\nand CSV as your exchange format.\n\n\n\n:::{.callout-note appearance=‚Äúsimple‚Äù}\nIn Short\n\n- CSV is universal but inefficient\n- Parquet is faster, smaller, and type-safe\n- Switching requires only {arrow}\n- Make Parquet your default for analysis projects\n:::\n\nModern workflows deserve modern storage.\n\n::: callout-tip\nIf you want to stay up to date with the latest events and posts from the Rome R Users Group:\n\nüëâ https://www.meetup.com/rome-r-users-group/\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}