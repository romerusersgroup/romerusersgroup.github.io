[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "We are committed to providing high-quality, accessible, and engaging educational resources that empower learners of all ages. Our mission is to foster a love for learning and to support the development of critical thinking skills through innovative and interactive content."
  },
  {
    "objectID": "about.html#our-mission",
    "href": "about.html#our-mission",
    "title": "About Us",
    "section": "",
    "text": "We are committed to providing high-quality, accessible, and engaging educational resources that empower learners of all ages. Our mission is to foster a love for learning and to support the development of critical thinking skills through innovative and interactive content."
  },
  {
    "objectID": "about.html#who-we-are",
    "href": "about.html#who-we-are",
    "title": "About Us",
    "section": "Who We Are?",
    "text": "Who We Are?\nWe believe education has the power to transform lives. Our team is dedicated to making learning accessible, meaningful, and inspiring for everyone.\n\n\nOrganizers Sponsors"
  },
  {
    "objectID": "about/former-organizers.html",
    "href": "about/former-organizers.html",
    "title": "former-organizers",
    "section": "",
    "text": "Lucy Michaels Organizer - from Rome. With 25 years in mathematics education and financial leadership, I‚Äôm completing a Master‚Äôs in Analysis & Modelling of Data & Processes as well as IBM‚Äôs Data Science Certificate, ready to pivot into Data Science. This has led me to work on hands-on projects using R, Python, SQL, and Excel. I bring a unique mix of analytical depth, cross-functional collaboration, and a love for problem-solving. After managing multi-million euro budgets and teaching complex maths, I am looking forward to transitioning into data science to turn messy, real-world data into clear, actionable insights. I‚Äôm also active in nonprofit leadership and public speaking, committed to using data‚Äîand storytelling‚Äîto make a difference. (June 2025 to Present)\n\n\n\n\n\n\nLucy Michaels"
  },
  {
    "objectID": "about/former-organizers.html#section",
    "href": "about/former-organizers.html#section",
    "title": "former-organizers",
    "section": "",
    "text": "Lucy Michaels Organizer - from Rome. With 25 years in mathematics education and financial leadership, I‚Äôm completing a Master‚Äôs in Analysis & Modelling of Data & Processes as well as IBM‚Äôs Data Science Certificate, ready to pivot into Data Science. This has led me to work on hands-on projects using R, Python, SQL, and Excel. I bring a unique mix of analytical depth, cross-functional collaboration, and a love for problem-solving. After managing multi-million euro budgets and teaching complex maths, I am looking forward to transitioning into data science to turn messy, real-world data into clear, actionable insights. I‚Äôm also active in nonprofit leadership and public speaking, committed to using data‚Äîand storytelling‚Äîto make a difference. (June 2025 to Present)\n\n\n\n\n\n\nLucy Michaels"
  },
  {
    "objectID": "about/organizers.html",
    "href": "about/organizers.html",
    "title": "Organizers",
    "section": "",
    "text": "Note\n\n\n\nThe Rome R Users Group was founded in 2025 by Federica Gazzelloni to connect and support R users, data scientists, and enthusiasts across Rome."
  },
  {
    "objectID": "about/organizers.html#section",
    "href": "about/organizers.html#section",
    "title": "Organizers",
    "section": "",
    "text": "Federica Gazzelloni Lead Organizer - from Rome. I am an independent researcher passionate about data science, with a background in actuarial statistics and expertise in collaborative environments. My journey extends from traditional actuarial work to advanced data science with R. As the lead organizer of R-Ladies Rome and Rome R Users Group, I advocate for inclusivity and knowledge-sharing, hosting various events promoting the R language but not limited to. I am also a book club facilitator with the DSLC (former R4DS) online community, fostering collaborative learning. Additionally, I have contributed as a reviewer to global health initiatives, emphasizing accessible learning and effective communication. (May 2025 to Present)\n\n\n\n\n\n\nFederica Gazzelloni"
  },
  {
    "objectID": "about/organizers.html#section-1",
    "href": "about/organizers.html#section-1",
    "title": "Organizers",
    "section": "",
    "text": "Lucio Colonna Organizer - from Rome. Data-Driven Professional. With over 10 years of experience in IT consulting and procurement, I combine business expertise with advanced data analysis skills. Proficient in R, Python, SQL, Power BI (including M and DAX), and Excel, I transform complex datasets into clear, actionable insights that drive strategic decisions. My background includes leading cost optimization initiatives and delivering ERP implementations, always with a data-driven approach. Passionate about problem-solving and storytelling with data, I bring both analytical depth and cross-functional collaboration to deliver lasting business impact. (September 2025 to Present)\n\n\n\n\n\n\nLucio Colonna"
  },
  {
    "objectID": "about/organizers.html#contact-us",
    "href": "about/organizers.html#contact-us",
    "title": "Organizers",
    "section": "Contact Us",
    "text": "Contact Us\nTo get in touch with Rome R Users Group, you can email us at romerusersgroup@gmail.com"
  },
  {
    "objectID": "about/organizers.html#code-of-conduct",
    "href": "about/organizers.html#code-of-conduct",
    "title": "Organizers",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nWe have a code of conduct that all members and participants are expected to follow. The code of conduct is designed to ensure that our community is a safe and welcoming space for everyone. We take any violations of the code of conduct seriously and will take appropriate action to address them."
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html",
    "href": "content/events/posts/27112025_jakubnowosad.html",
    "title": "R for geospatial predictive mapping",
    "section": "",
    "text": "Registered Attendees (203)"
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html#overview",
    "href": "content/events/posts/27112025_jakubnowosad.html#overview",
    "title": "R for geospatial predictive mapping",
    "section": "Overview",
    "text": "Overview\nOn November 27, the Rome R Users Group hosted a special workshop led by Jakub Nowosad, one of the most recognized voices in the R spatial community. More than 200 participants registered for the event, marking one of our largest and most engaging sessions to date. For those who couldn‚Äôt attend live, or who wish to revisit the material‚Äîthis page includes the recording and all essential information so you can follow the workshop at your own pace.\n\nA Deep Dive into Geospatial Predictive Mapping\nThe workshop focused on practical workflows for building reliable spatial predictions in R. Much of the discussion revolved around the challenges of predicting spatial patterns in real-world contexts, where data often contain gaps, biases, or irregular sampling.\nJakub demonstrated how R can be used not only to model such data but also to valuate model performance spatially and visualize results in ways that highlight both strengths and limitations. Concepts like spatial cross-validation, the Area of Applicability (AoA), and prediction-domain adaptive diagnostics were explained clearly, illustrating why traditional modeling approaches often fall short when applied to geographic problems.\nThroughout the session, participants followed concrete examples showing how various R packages, such as {sf}, {terra}, {CAST} and several visualization tools, including {tmap}, work together within a workflow. The emphasis is on reproducibility and transparent code. The workshop is particularly valuable for practitioners working in environmental analysis, ecology, remote sensing, and related fields."
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html#about-the-speaker",
    "href": "content/events/posts/27112025_jakubnowosad.html#about-the-speaker",
    "title": "R for geospatial predictive mapping",
    "section": "About the Speaker",
    "text": "About the Speaker\nJakub Nowosad is an Associate Professor at the Adam Mickiewicz University in Poznan and a Visiting Scientist at University of M√ºnster, specializing in geospatial data analysis and R programming. He is the author of Geocomputation with R, a comprehensive resource for spatial data science in R. A Python version of the book has also been released recently and can be found at https://py.geocompx.org/. Jakub has developed many R packages for spatial data manipulation and visualization, and has contributed extensively to the R community through workshops, tutorials, and publications."
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html#watch-the-recording",
    "href": "content/events/posts/27112025_jakubnowosad.html#watch-the-recording",
    "title": "R for geospatial predictive mapping",
    "section": "Watch the Recording",
    "text": "Watch the Recording\nIf you missed the live workshop, you can watch the recording here:\nüé¨ Watch the Video Now\nSimply click on the video below to catch up on the discussion:\n\nThe recording includes the full demonstration, slides, and the audience Q&A session."
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html#materials-and-resources",
    "href": "content/events/posts/27112025_jakubnowosad.html#materials-and-resources",
    "title": "R for geospatial predictive mapping",
    "section": "Materials and Resources",
    "text": "Materials and Resources\nYou can access the workshop materials, including slides and code examples, on Jakub Nowosad‚Äôs website:\n\nWorkshop Materials\n\n\n\n\n\n\n\nTip\n\n\n\nNote: The materials include all R scripts and datasets used during the workshop, allowing you to follow along and practice the techniques demonstrated.\n\nUse the left/right arrow keys on your keyboard to navigate the presentation.\n\nTo explore the code used in the workshop, click the arrow button on the side of the slides."
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html#learning-objectives",
    "href": "content/events/posts/27112025_jakubnowosad.html#learning-objectives",
    "title": "R for geospatial predictive mapping",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this workshop, participants will be able to: - Build predictive models using spatial data - Validate the results in a robust way - Visualize geospatial data and model outputs effectively"
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html#topics-covered",
    "href": "content/events/posts/27112025_jakubnowosad.html#topics-covered",
    "title": "R for geospatial predictive mapping",
    "section": "Topics Covered",
    "text": "Topics Covered\n\nIntroduction to geocomputational methods\nBuilding predictive models using spatial data\nValidating the models results\nVisualization techniques for geospatial data"
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html#what-participants-learned",
    "href": "content/events/posts/27112025_jakubnowosad.html#what-participants-learned",
    "title": "R for geospatial predictive mapping",
    "section": "What Participants Learned",
    "text": "What Participants Learned\nRather than treating geospatial modeling as a sequence of isolated steps, Jakub encouraged a holistic perspective. Participants saw how the process begins with understanding the structure and quality of spatial data, continues through careful model training and selection, and culminates in interpreting predictions within their geographical context.\nParticular attention was given to the pitfalls that arise when spatial structure is ignored‚Äîfor example, overly optimistic accuracy metrics or unrealistic prediction maps. Jakub explained how spatially aware resampling strategies lead to more reliable evaluations and how tools like the AoA metric help determine where a model should or should not be trusted.\nVisualisation was another central theme. The workshop demonstrated how thoughtful map design can communicate both predictions and associated uncertainties, promoting transparent and responsible modeling."
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html#who-is-this-workshop-for",
    "href": "content/events/posts/27112025_jakubnowosad.html#who-is-this-workshop-for",
    "title": "R for geospatial predictive mapping",
    "section": "Who Is This Workshop For?",
    "text": "Who Is This Workshop For?\nThis workshop is ideal for data scientists, GIS professionals, and R users interested in advancing their skills in geospatial predictive mapping. A basic understanding of R programming and spatial data concepts is recommended."
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html#what-do-you-need-to-follow-along",
    "href": "content/events/posts/27112025_jakubnowosad.html#what-do-you-need-to-follow-along",
    "title": "R for geospatial predictive mapping",
    "section": "What Do You Need to Follow Along?",
    "text": "What Do You Need to Follow Along?\nParticipants interested in reproducing the code demonstrated during the workshop should ensure that, in addition to having R and RStudio installed, the following R packages are available:\n\nsf\n\nterra\n\ncaret\n\nCAST\ntmap"
  },
  {
    "objectID": "content/events/posts/27112025_jakubnowosad.html#contact",
    "href": "content/events/posts/27112025_jakubnowosad.html#contact",
    "title": "R for geospatial predictive mapping",
    "section": "Contact",
    "text": "Contact\nFor more information about the workshop, please contact us at romerusersgroup@gmail.com.\n\n‚ÄúWe look forward to seeing you there!‚Äù"
  },
  {
    "objectID": "content/events/posts/01062025_inperson.html",
    "href": "content/events/posts/01062025_inperson.html",
    "title": "Rome in Person Meetup",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/rhacks/index.html#section",
    "href": "content/rhacks/index.html#section",
    "title": "R-Hacks",
    "section": "",
    "text": "R-Hacks specialises in short, focused examples that solve a concrete problem. No long tutorials, no heavy theory‚Äîjust the exact piece of code you need right now. From quick ggplot adjustments to clever data wrangling patterns, each snippet is built to be copied, adapted, and used immediately in your own analysis.\nBeyond simple tricks, the blog highlights efficient ways of thinking about data. You‚Äôll find micro-workflows that combine tidyverse tools, reproducible visualisation techniques, and small utilities that make routine tasks easier. Every post aims to help you write cleaner, faster, and more robust R code by adopting sensible, modern practices.\nR-Hacks is powered by the Rome R Users Group community. We collect clever ideas, handy functions, and small daily discoveries shared during meetups or projects. The goal is to create a living library of high-impact examples that reflect what people actually use in their analysis‚Äînot theoretical snippets, but real solutions to real problems."
  },
  {
    "objectID": "content/rhacks/posts/04-01232026/index.html",
    "href": "content/rhacks/posts/04-01232026/index.html",
    "title": "Showing Top & Bottom values in one clear visualization",
    "section": "",
    "text": "This hack is based on my analysis on Kaggle analysis linked as follows (please see Chapter 4.3):\nüîó https://www.kaggle.com/code/lcolon/exploring-2024-software-engineer-salaries\nWhen exploring a distribution, a very common question is:\nShowing only the Top values might hid some context, while showing two separate charts can make comparison harder.\nThis R-Hack shows a simple and reusable pattern to display Top and Bottom values together in a single, clean visualization, as the one showed below:"
  },
  {
    "objectID": "content/rhacks/posts/04-01232026/index.html#step-0-create-an-example-dataset",
    "href": "content/rhacks/posts/04-01232026/index.html#step-0-create-an-example-dataset",
    "title": "Showing Top & Bottom values in one clear visualization",
    "section": "Step 0 ‚Äì Create an example dataset",
    "text": "Step 0 ‚Äì Create an example dataset\nLet‚Äôs start from a small, simulated dataset representing average salaries for different companies:\n\nlibrary(tidyverse)\n\nset.seed(123)\n\ndf &lt;- data.frame(\n  company = (LETTERS[1:26]),\n  avg_salary = round(runif(26, min = 60, max = 180), 0)\n)\n\nhead(df)\n\n  company avg_salary\n1       A         95\n2       B        155\n3       C        109\n4       D        166\n5       E        173\n6       F         65"
  },
  {
    "objectID": "content/rhacks/posts/04-01232026/index.html#step-1-build-top-bottom-datasets-in-a-single-pipeline",
    "href": "content/rhacks/posts/04-01232026/index.html#step-1-build-top-bottom-datasets-in-a-single-pipeline",
    "title": "Showing Top & Bottom values in one clear visualization",
    "section": "Step 1 ‚Äì Build Top & Bottom datasets in a single pipeline",
    "text": "Step 1 ‚Äì Build Top & Bottom datasets in a single pipeline\nIn this step, we extract both the Top 10 and Bottom 10 values using a single, readable pipeline.\nThe idea is simple. Starting from the same dataset:\n\nselect the Top 10 by applying slice_max(), which returns all observations within the top 10 ranking positions, including any ties\nselect the Bottom 10 by applying slice_min(), returning all observations within the lowest 10 ranking positions, again including ties\nassign a clear group label to each subset\ncombine the two subsets into a single dataframe for visualization and further analysis\n\n\nplot_df &lt;- \n  bind_rows(\n    df %&gt;% \n      slice_max(avg_salary, n = 10, with_ties = TRUE) %&gt;%\n      mutate(group = \"Top 10\"),\n    \n    df %&gt;% \n      slice_min(avg_salary, n = 10, with_ties = TRUE) %&gt;%\n      mutate(group = \"Bottom 10\")\n  )\n\nhead(plot_df)\n\n  company avg_salary  group\n1       X        179 Top 10\n2       K        175 Top 10\n3       T        175 Top 10\n4       E        173 Top 10\n5       P        168 Top 10\n6       H        167 Top 10"
  },
  {
    "objectID": "content/rhacks/posts/04-01232026/index.html#step-2---plot-the-data",
    "href": "content/rhacks/posts/04-01232026/index.html#step-2---plot-the-data",
    "title": "Showing Top & Bottom values in one clear visualization",
    "section": "Step 2 - Plot the data",
    "text": "Step 2 - Plot the data\nIn this step, starting from the dataframe created in the previous step, we build a simple visualization with ggplot:\n\nplot_base &lt;- ggplot(plot_df, \n                    aes(x = reorder(company, -avg_salary), y = avg_salary)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = avg_salary), vjust = -0.4, size = 4) +\n  facet_wrap(~ fct_rev(group), scales = \"free_x\") +\n  labs(\n    title = \"Top and Bottom Companies by Average Salary\",\n    subtitle = \"Simulated data example\\n\\n\",\n    y = \"Average yearly salary\",\n    x = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    axis.text.x = element_text(size = 10),\n    axis.text.y = element_blank()\n  )\n\nplot_base"
  },
  {
    "objectID": "content/rhacks/posts/04-01232026/index.html#step-3---generate-gradient-color-palettes",
    "href": "content/rhacks/posts/04-01232026/index.html#step-3---generate-gradient-color-palettes",
    "title": "Showing Top & Bottom values in one clear visualization",
    "section": "Step 3 - Generate gradient color palettes",
    "text": "Step 3 - Generate gradient color palettes\nNow that the base chart is working, we can make it more informative by adding two gradient color palettes:\n\none gradient for the Top group (blue shades)\none gradient for the Bottom group (red shades)\n\nInstead of assigning a single color per group, we assign a slightly different shade to each bar. This creates a clean gradient effect while keeping the plot readable ‚Äî even when ties produce more than 10 observations per group.\nTo do this, we:\n\ngenerate a group-specific palette with colorRampPalette(), sized to the number of rows in each group (so it adapts if ties expand the selection)\ngroup the data by group (Top vs Bottom)\nassign colors row by row using row_number()\n\nstore the result in a new column called color\n\n\n\nplot_df &lt;- plot_df %&gt;%\n  group_by(group) %&gt;%\n  mutate(\n    color = if_else(\n      group == \"Top 10\",\n      colorRampPalette(c(\"darkblue\", \"lightblue\"))(n())[row_number()],\n      colorRampPalette(c(\"darkred\", \"lightcoral\"))(n())[row_number()]\n    )\n  ) %&gt;%\n  ungroup()\n\nhead(plot_df)\n\n# A tibble: 6 √ó 4\n  company avg_salary group  color  \n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  \n1 X              179 Top 10 #00008B\n2 K              175 Top 10 #131895\n3 T              175 Top 10 #26309F\n4 E              173 Top 10 #3948A9\n5 P              168 Top 10 #4C60B3\n6 H              167 Top 10 #6078BD"
  },
  {
    "objectID": "content/rhacks/posts/04-01232026/index.html#step-4---style-the-chart-using-the-new-colors",
    "href": "content/rhacks/posts/04-01232026/index.html#step-4---style-the-chart-using-the-new-colors",
    "title": "Showing Top & Bottom values in one clear visualization",
    "section": "Step 4 - Style the chart using the new colors",
    "text": "Step 4 - Style the chart using the new colors\nAt this point, we don‚Äôt want to rewrite the entire ggplot call. Instead, we:\n\nreuse the base chart (plot_base)\nreplace its dataset with the updated plot_df (the one that now includes color) using the %+% operator\nadd a new geom_col() that maps fill to the color column\nuse scale_fill_identity() so ggplot uses the colors as they are listed in the color column\n\n\nplot_styled &lt;- (plot_base %+% plot_df) +\n  geom_col(aes(fill = color)) +\n  scale_fill_identity()\n\nplot_styled"
  },
  {
    "objectID": "content/rhacks/posts/04-01232026/index.html#in-short",
    "href": "content/rhacks/posts/04-01232026/index.html#in-short",
    "title": "Showing Top & Bottom values in one clear visualization",
    "section": "In short",
    "text": "In short\n\nExtract Top 10 and Bottom 10 values from the same dataset using slice_max() and slice_min(), including ties\nCombine the two subsets into a single dataframe for a compact overview of the distribution extremes\nBuild a clean base plot to validate structure and layout\nEnhance the visualization by applying gradient colors to distinguish Top and Bottom groups\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you want to stay up to date with the latest events from the Rome R Users Group, click here:\nüëâ https://www.meetup.com/rome-r-users-group/\nAnd if you are curious, the full Kaggle notebook used for this tip is available here:\nüîó https://www.kaggle.com/code/lcolon/exploring-2024-software-engineer-salaries"
  },
  {
    "objectID": "content/rhacks/posts/07-02152026/index.html",
    "href": "content/rhacks/posts/07-02152026/index.html",
    "title": "Stop Copy‚ÄìPasting. Start Iterating.",
    "section": "",
    "text": "After fixing structure (N.3), adding sanity checks (N.5), and reshaping data properly (N.6), the next natural step is iteration over clean data.\nIn R, the problem is rarely how to write a loop. The real question is:\nThis R-Hack focuses on replacing duplication with controlled iteration."
  },
  {
    "objectID": "content/rhacks/posts/07-02152026/index.html#the-core-problem-duplication",
    "href": "content/rhacks/posts/07-02152026/index.html#the-core-problem-duplication",
    "title": "Stop Copy‚ÄìPasting. Start Iterating.",
    "section": "The Core Problem: Duplication",
    "text": "The Core Problem: Duplication\nYou see code like this:\n\nmean(df$var1)\nmean(df$var2)\nmean(df$var3)\n\nOr:\n\nplot(df$year_2020)\nplot(df$year_2021)\nplot(df$year_2022)\n\nIt works.\n\n\n\n\n\n\n\nBut it does not scale\nIt is fragile\nAnd it invites mistakes\n\n\n\n\n\nDuplication is your signal that iteration may be appropriate.\n\nStep 1 ‚Äì Simulate Example Data\n\nset.seed(123)\n\ndf &lt;- data.frame(\n  var1 = rnorm(100),\n  var2 = rnorm(100, mean = 2),\n  var3 = rnorm(100, mean = 4)\n)\n\nhead(df)\n\n         var1     var2     var3\n1 -0.56047565 1.289593 6.198810\n2 -0.23017749 2.256884 5.312413\n3  1.55870831 1.753308 3.734855\n4  0.07050839 1.652457 4.543194\n5  0.12928774 1.048381 3.585660\n6  1.71506499 1.954972 3.523753\n\n\nStep 2 ‚Äì A Clean for Loop\nInstead of repeating logic, define the variables once.\n\nvars &lt;- names(df)\n\nfor (v in vars) {\n  print(mean(df[[v]], na.rm = TRUE))\n}\n\n[1] 0.09040591\n[1] 1.892453\n[1] 4.120465\n\n\nThis loop:\n\niterates over column names\nextracts values using [[ ]]\navoids copy‚Äìpaste\nkeeps logic consistent\n\nSimple and readable.\nStep 3 ‚Äì Storing Results Properly (Preallocation)\nIf you need results for later use, preallocate.\n\nresults &lt;- numeric(length(vars))\n\nfor (i in seq_along(vars)) {\n  results[i] &lt;- mean(df[[vars[i]]], na.rm = TRUE)\n}\n\nresults\n\n[1] 0.09040591 1.89245320 4.12046511\n\n\nWhy preallocate?\n\navoids growing objects inside the loop\nkeeps structure explicit\nimproves clarity and performance\n\n\nPreallocation is not about micro-optimisation. It is about good structure.\n\nStep 4 ‚Äì Plotting Without Copy‚ÄìPaste\n‚ùå The Copy‚ÄìPaste Pattern\nplot(df$var1)\nplot(df$var2)\nplot(df$var3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis works ‚Äî but does not scale.\n‚úÖ Base R Loop\nfor (v in vars) {\n  hist(df[[v]],\n       main = paste(\"Histogram of\", v),\n       col = \"steelblue\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis produces one histogram per variable.\n‚úÖ ggplot2 Loop (Important Detail)\nlibrary(ggplot2)\n\nfor (v in vars) {\n  p &lt;- ggplot(df, aes(x = .data[[v]])) +\n    geom_histogram(fill = \"steelblue\", bins = 20) +\n    labs(title = paste(\"Histogram of\", v)) +\n    theme_minimal()\n\n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüîé Important: print(p) is required inside loops. Without it, nothing is displayed.\n\n\n\nThis is a subtle but crucial detail many users miss."
  },
  {
    "objectID": "content/rhacks/posts/07-02152026/index.html#when-not-to-loop",
    "href": "content/rhacks/posts/07-02152026/index.html#when-not-to-loop",
    "title": "Stop Copy‚ÄìPasting. Start Iterating.",
    "section": "When Not to Loop",
    "text": "When Not to Loop\nIf you are looping over columns, pause. Ask:\n\nShould the data be long instead?\n\nFor example (see R-Hacks N.6):\n\nlibrary(tidyr)\n\ndf_long &lt;- df |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  )\n\nggplot(df_long, aes(value)) +\n  geom_histogram(bins = 20, fill = \"steelblue\") +\n  facet_wrap(~ variable, scales = \"free\") +\n  theme_minimal()\n\n\n\n\n\n\n\nSometimes reshaping simplifies everything more than iterating."
  },
  {
    "objectID": "content/rhacks/posts/07-02152026/index.html#editorial-stance",
    "href": "content/rhacks/posts/07-02152026/index.html#editorial-stance",
    "title": "Stop Copy‚ÄìPasting. Start Iterating.",
    "section": "Editorial Stance",
    "text": "Editorial Stance\nLoops are not bad. Copy‚Äìpaste is worse.\nThe goal is not to write clever code. The goal is to write less duplicated code.\nIteration becomes powerful when:\n\nstructure is clean\nshape is correct\nlogic is stable\n\n\n\n\n\n\n\nNote\n\n\n\nIn Short\n\nDuplication is a signal\nReplace repetition with controlled iteration\nPreallocate when storing results\nRemember print() inside ggplot loops\nIf you are looping across columns, reconsider the data shape\n\n\n\nIteration is not about complexity. It is about discipline.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to stay up to date with the latest events and posts from the Rome R Users Group:\nüëâ https://www.meetup.com/rome-r-users-group/"
  },
  {
    "objectID": "content/rhacks/posts/06-02082026/index.html#early-warning-signs-you-should-pivot",
    "href": "content/rhacks/posts/06-02082026/index.html#early-warning-signs-you-should-pivot",
    "title": "When to Pivot: Spotting Data Shape Problems Early",
    "section": "Early warning signs you should pivot",
    "text": "Early warning signs you should pivot\nIf you see any of the following, pause and consider reshaping:\n\n\nmutate() or summarise() repeated across many columns\n\nacross(starts_with(...)) doing heavy lifting\n\nggplot code that manually references multiple variables\ncolumn names encoding values (e.g.¬†sales_2022, sales_2023)\ncopy‚Äìpaste patterns that differ only by column name\n\nThese are shape problems, not syntax problems."
  },
  {
    "objectID": "content/rhacks/posts/06-02082026/index.html#a-minimal-example",
    "href": "content/rhacks/posts/06-02082026/index.html#a-minimal-example",
    "title": "When to Pivot: Spotting Data Shape Problems Early",
    "section": "A minimal example",
    "text": "A minimal example\nLet‚Äôs simulate a small, tidy-looking dataset that is actually awkward to work with.\n\nlibrary(dplyr)\nlibrary(tidyr)\n\ndf_wide &lt;- data.frame(\n  id = 1:5,\n  glucose_morning = c(110, 98, 130, 105, 120),\n  glucose_evening = c(140, 115, 160, 125, 150)\n)\n\ndf_wide\n\n  id glucose_morning glucose_evening\n1  1             110             140\n2  2              98             115\n3  3             130             160\n4  4             105             125\n5  5             120             150\n\n\n\n\n\n\n\n\nAt first glance, this looks fine. But now try to:\n\ncompute summaries\nmake a single plot\ncompare morning vs evening values\n\n\n\n\nEverything becomes more complicated than necessary."
  },
  {
    "objectID": "content/rhacks/posts/06-02082026/index.html#pivot-once-simplify-everything",
    "href": "content/rhacks/posts/06-02082026/index.html#pivot-once-simplify-everything",
    "title": "When to Pivot: Spotting Data Shape Problems Early",
    "section": "Pivot once, simplify everything",
    "text": "Pivot once, simplify everything\nA single pivot_longer() changes the workflow completely.\n\ndf_long &lt;- df_wide |&gt;\n  pivot_longer(\n    cols = starts_with(\"glucose\"),\n    names_to = \"time\",\n    values_to = \"glucose\"\n  )\n\ndf_long\n\n# A tibble: 10 √ó 3\n      id time            glucose\n   &lt;int&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1     1 glucose_morning     110\n 2     1 glucose_evening     140\n 3     2 glucose_morning      98\n 4     2 glucose_evening     115\n 5     3 glucose_morning     130\n 6     3 glucose_evening     160\n 7     4 glucose_morning     105\n 8     4 glucose_evening     125\n 9     5 glucose_morning     120\n10     5 glucose_evening     150\n\n\nNow each row is a measurement, not a column variant.\n\nSee the payoff immediately\n\nSummaries become simpler\n\ndf_long |&gt;\n  group_by(time) |&gt;\n  summarise(\n    n = n(),\n    mean_glucose = mean(glucose)\n  )\n\n# A tibble: 2 √ó 3\n  time                n mean_glucose\n  &lt;chr&gt;           &lt;int&gt;        &lt;dbl&gt;\n1 glucose_evening     5         138 \n2 glucose_morning     5         113.\n\n\nPlots become trivial\n\nlibrary(ggplot2)\n\nggplot(df_long, aes(x = time, y = glucose)) +\n  geom_boxplot() +\n  labs(\n    title = \"Glucose levels by time of day\",\n    x = NULL,\n    y = \"Glucose\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNo manual aesthetics\nNo duplicated code\nNo workarounds\n\n\nWhen not to pivot\n\nNot everything should be long.\n\n\n\n\n\n\nAvoid pivoting when:\n\ncolumns represent different concepts (e.g.¬†age vs height)\nyou are producing final reporting tables\nidentifiers would be duplicated unnecessarily\n\n\n\n\nPivoting is a tool, not a rule."
  },
  {
    "objectID": "content/rhacks/posts/06-02082026/index.html#making-this-a-habit",
    "href": "content/rhacks/posts/06-02082026/index.html#making-this-a-habit",
    "title": "When to Pivot: Spotting Data Shape Problems Early",
    "section": "Making this a habit",
    "text": "Making this a habit\nBefore adding complexity, ask: - Am I repeating the same logic across columns? - Am I fighting ggplot instead of feeding it clean data? - Are column names doing the job rows should be doing?\nIf the answer is ‚Äúyes‚Äù, try reshaping early.\n\n\n\n\n\n\nIn short\n\nMany messy workflows start with data in the wrong shape\nShape problems masquerade as logic problems\nOne early pivot can remove dozens of lines of code\nDecide structure early, then move on\n\n\n\n\nThis is not about writing more code. It‚Äôs about writing less."
  },
  {
    "objectID": "content/rhacks/posts/06-02082026/index.html#industry-update-short-note",
    "href": "content/rhacks/posts/06-02082026/index.html#industry-update-short-note",
    "title": "When to Pivot: Spotting Data Shape Problems Early",
    "section": "Industry update (short note)",
    "text": "Industry update (short note)\nPosit has opened a private beta of an AI assistant embedded directly in RStudio. If you‚Äôre curious to try it, you can join the waiting list here:\nüëâ https://posit.co/products/enterprise/ai/\n\n\n\n\n\n\nTip\n\n\n\nIf you want to stay up to date with the latest events and posts from the Rome R Users Group, follow us here:\nüëâ https://www.meetup.com/rome-r-users-group/"
  },
  {
    "objectID": "content/rhacks/posts/05-02022026/index.html",
    "href": "content/rhacks/posts/05-02022026/index.html",
    "title": "One-Line Sanity Checks After Every Transformation",
    "section": "",
    "text": "This hack is based on RLadiesRome Tutorial analysis on From Basics to Advanced Health Analytics: Exploring Diabetes Data:\nüîó https://rladiesrome.github.io/Principles-of-data-Analysis-in-R/\nWhen working with data, most bugs don‚Äôt come from complex models.\nThis R-Hack shows how to build a habit of running one-line sanity checks after every major transformation, using a concrete example from our Exploring Diabetes Data tutorial.\nThey are quick, cheap, and often enough to catch problems before plots or models hide them."
  },
  {
    "objectID": "content/rhacks/posts/05-02022026/index.html#why-one-line-checks-matter",
    "href": "content/rhacks/posts/05-02022026/index.html#why-one-line-checks-matter",
    "title": "One-Line Sanity Checks After Every Transformation",
    "section": "Why One-Line Checks Matter",
    "text": "Why One-Line Checks Matter\nTransformations are the most fragile part of a data workflow. They change structure, size, and meaning ‚Äî often without throwing an error.\nThe goal here is not defensive programming or heavy validation. It‚Äôs about building small, automatic pauses that let you ask:\n\n‚ÄúDoes the data still look the way I expect?‚Äù\n\nData Example: Diabetes Dataset (Before Clustering)\nIn the tutorial, the diabetes dataset is cleaned and prepared before moving to k-prototypes clustering.\nAt that point, the data typically goes through steps like:\n\nselecting relevant variables\nrecoding categorical fields\nfiltering incomplete observations\n\nThat makes it an ideal place to pause and check assumptions. Let‚Äôs simulate a sample dataset similar to the one used in the tutorial:\n\nlibrary(dplyr)\n\nset.seed(123)\n\ndf_before &lt;- data.frame(\n  id       = 1:300,\n  age      = round(rnorm(300, mean = 55, sd = 10)),\n  bmi      = round(rnorm(300, mean = 28, sd = 5), 1),\n  glucose  = round(rnorm(300, mean = 120, sd = 30)),\n  sex      = sample(c(\"Female\", \"Male\"), 300, replace = TRUE),\n  diabetes = sample(c(\"No\", \"Yes\"), 300, replace = TRUE, prob = c(0.7, 0.3))\n)\n\n# Introduce realistic missingness (so filtering has an effect)\nset.seed(456)\ndf_before$bmi[sample(seq_len(nrow(df_before)), size = 20)] &lt;- NA\ndf_before$glucose[sample(seq_len(nrow(df_before)), size = 15)] &lt;- NA\n\n\ndim(df_before)\n\n[1] 300   6\n\n\n\nhead(df_before)\n\n  id age  bmi glucose    sex diabetes\n1  1  49 24.4     152   Male       No\n2  2  53 24.2     119   Male       No\n3  3  71 23.3     119   Male       No\n4  4  56 22.7      75 Female      Yes\n5  5  56 25.8     144   Male       No\n6  6  72 29.7     114 Female      Yes\n\n\nAfter the transformations, we get a new dataframe df_after:\n\ndf_after &lt;- df_before |&gt;\n  dplyr::filter(!is.na(glucose), !is.na(bmi)) |&gt;\n  dplyr::mutate(\n    diabetes = factor(diabetes, levels = c(\"No\", \"Yes\"))\n  )\n\ndim(df_after)\n\n[1] 266   6\n\n\n\nhead(df_after)\n\n  id age  bmi glucose    sex diabetes\n1  1  49 24.4     152   Male       No\n2  2  53 24.2     119   Male       No\n3  3  71 23.3     119   Male       No\n4  4  56 22.7      75 Female      Yes\n5  5  56 25.8     144   Male       No\n6  6  72 29.7     114 Female      Yes\n\n\nCheck 1 ‚Äì Did the Row Count Change as Expected?\nAfter any operation that could affect the number of rows, check it explicitly.\n\n# One-line sanity check: did we drop rows as expected?\nnrow(df_before)\n\n[1] 300\n\nnrow(df_after)\n\n[1] 266\n\n\n\n\n\n\n\n\nNotes (why this works)\n\nWe create missing values in bmi and glucose, which is common in real health datasets.\nThe filter step now drops incomplete rows, so df_after has fewer rows than df_before.\nThis makes your ‚Äúrow count changed‚Äù check meaningful and aligned with Issue N.5‚Äôs message.\n\n\n\n\nIn this context, a reduction is expected, but it should be understood and intentional.\nUse this after:\n\nfilter()\njoin()\nsubsetting\nremoving duplicates\n\nWhat this catches:\n\naccidental row loss\nmany-to-many joins\naccidental over-filtering\nunintended row drops\nsilent data duplication\n\nIf the number changes unexpectedly, stop and investigate. This single check prevents a large class of downstream errors.\nCheck 2 ‚Äì Do Key Variables Still Look Reasonable?\nWhenever you create or transform a variable, inspect its range.\n\nsummary(df_after$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   32.0    49.0    55.0    55.4    61.0    87.0 \n\n\n\nrange(df_after$bmi, na.rm = TRUE)\n\n[1] 14.0 40.9\n\n\nLook for:\n\nnegative values where they shouldn‚Äôt exist\nimpossible values (e.g.¬†BMI of 5)\nsuspicious defaults (e.g.¬†all zeros)\ntransformations that didn‚Äôt behave as expected\n\nThese problems are much easier to fix immediately than after modelling.\nCheck 3 ‚Äì Did Missingness Increase?\nJoins and reshaping often introduce NAs. Count them explicitly.\n\ncolSums(is.na(df_after))\n\n      id      age      bmi  glucose      sex diabetes \n       0        0        0        0        0        0 \n\n\nThis gives a fast overview of:\n\ncolumns that need cleaning\nvariables that may bias analysis\nfields that should be dropped or imputed\n\nNever assume ‚Äúthere aren‚Äôt many NAs‚Äù. Check!\nCheck 4 ‚Äì Do Group Sizes Still Make Sense?\nBefore summarising or modelling grouped data, look at the group counts. In particular, before using categorical variables in clustering or summaries, check their distribution.\n\ndf_after |&gt; dplyr::count(diabetes)\n\n  diabetes   n\n1       No 188\n2      Yes  78\n\n\n\n\nThis is especially important before:\n\nrates or percentages\naverages by group\ncomparisons across categories\n\n\nThis prevents:\n\nunstable clusters\ncategories with too few observations\nmisleading interpretations\n\n\n\nWhat this catches:\n\ntiny or empty groups\nunstable estimates\nsummaries that look precise but aren‚Äôt meaningful"
  },
  {
    "objectID": "content/rhacks/posts/05-02022026/index.html#making-this-a-habit",
    "href": "content/rhacks/posts/05-02022026/index.html#making-this-a-habit",
    "title": "One-Line Sanity Checks After Every Transformation",
    "section": "Making This a Habit",
    "text": "Making This a Habit\nThese checks are not meant to clutter your script. They are meant to become automatic.\nA good rule of thumb:\n\nAfter anything that changes rows ‚Üí check nrow()\n\nAfter anything that changes values ‚Üí check summary() or range()\n\nAfter anything that combines data ‚Üí check NAs\n\nBefore summarising ‚Üí check group sizes\n\nOne line is often enough.\n\n\n\n\n\n\nIn Short\n\nMost data bugs enter during transformations\nOne-line checks catch problems early\nThey cost seconds and save hours\nMake them a habit, not an afterthought\n\n\n\n\nThis is not about perfection. It‚Äôs about seeing problems while they‚Äôre still small.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you want to stay up to date with the latest events and posts from the Rome R Users Group, follow us here:\nüëâ https://www.meetup.com/rome-r-users-group/"
  },
  {
    "objectID": "content/news/index.html",
    "href": "content/news/index.html",
    "title": "R Consortium Events & Workshops",
    "section": "",
    "text": "Stay updated with the latest news and resources from the R community. Here are some valuable links to keep you informed and connected:"
  },
  {
    "objectID": "content/news/index.html#news",
    "href": "content/news/index.html#news",
    "title": "R Consortium Events & Workshops",
    "section": "",
    "text": "Stay updated with the latest news and resources from the R community. Here are some valuable links to keep you informed and connected:"
  },
  {
    "objectID": "content/news/index.html#check-recent-posts-from-selected-r-and-data-science-sources",
    "href": "content/news/index.html#check-recent-posts-from-selected-r-and-data-science-sources",
    "title": "R Consortium Events & Workshops",
    "section": "Check recent posts from selected R and data science sources:",
    "text": "Check recent posts from selected R and data science sources:\n\n\n\n\n\n\nDate\nSource\nTitle\n\n\n\n\n2026-02-23\nRWeekly.org - Blogs to Learn R from the Community\nR Weekly 2026-W09 Git commits, Pick a License, and How to choose the best LLM using R and vitals\n\n\n\n2026-02-16\nR-Ladies Rome\nA Year of Learning, Growth, and Community at R-Ladies Rome üåü\n\n\n\n2026-02-16\nRWeekly.org - Blogs to Learn R from the Community\nR Weekly 2026-W08 submissions working group, nested Lists with xfun::tabset\n\n\n\n2026-02-15\nR-Ladies Rome\nFrom Data to Decisions: Digital Twins for Smarter Maintenance\n\n\n\n2026-02-09\nRWeekly.org - Blogs to Learn R from the Community\nR Weekly 2026-W07 sitrep, webRios, Jarl linter\n\n\n\n2026-02-02\nRWeekly.org - Blogs to Learn R from the Community\nR Weekly 2026-W06 Interview, Practical git, duckdb\n\n\n\n2026-01-28\nR-Ladies Rome\nGit & GitHub: Practical Version Control for Data Work\n\n\n\n2026-01-26\nRWeekly.org - Blogs to Learn R from the Community\nR Weekly 2026-W05 LLMs & plots, soil flux, futurize\n\n\n\n2026-01-20\nRWeekly.org - Blogs to Learn R from the Community\nR Weekly 2026-W04 Automate PR‚Äôs with Claude, Accessible line charts\n\n\n\n2026-01-12\nRWeekly.org - Blogs to Learn R from the Community\nR Weekly 2026-W03 R & Python Pluralism, RAG Setup, tinyshinyserver"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 Rome-R-Users-Group authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/resources/index.html",
    "href": "content/resources/index.html",
    "title": "Rome R-Users Group",
    "section": "",
    "text": "Some recommended R books‚Ä¶\n\nR for Data Science by Hadley Wickham & Garrett Grolemund - A comprehensive guide to data science using R and the tidyverse.\n\nAdvanced R by Hadley Wickham - A deep dive into the R programming language and its advanced features.\nThe Art of R Programming by Norman Matloff - A thorough introduction to R programming for beginners and experienced programmers alike.\nR in Action by Robert Kabacoff - A practical guide to R with numerous examples and case studies.\nData Science with R by Garrett Grolemund & Hadley Wickham - A hands-on guide to data science using R.\nPractical Statistics for Data Scientists by Peter Bruce & Andrew Bruce - A practical guide to statistical methods for data science using R.\n\n\n\n\nList of R packages containers‚Ä¶\n\nCRAN - The Comprehensive R Archive Network, the primary repository for R packages.\nBioconductor - A repository for bioinformatics and computational biology packages in R.\nRopensci - A collection of R packages for open science and reproducible research.\n\n\n\n\nLinks to tutorials‚Ä¶\n\nRStudio Education - A variety of tutorials and resources for learning R and RStudio.\nSwirl - An interactive learning platform that teaches R programming and data science directly in the R console.\nDataCamp - Online courses and tutorials for learning R and data science.\nCoursera R Courses - A variety of R programming courses available on Coursera.\nYouTube Channels - Numerous channels and videos dedicated to R programming and data science.\n\n\n\n\nUseful references‚Ä¶\n\nRStudio Cheatsheets - Handy reference sheets for various R packages and functions.\nCRAN Task Views - Curated lists of R packages for specific topics.\nTidyverse - A collection of R packages designed for data science."
  },
  {
    "objectID": "content/resources/index.html#resources",
    "href": "content/resources/index.html#resources",
    "title": "Rome R-Users Group",
    "section": "",
    "text": "Some recommended R books‚Ä¶\n\nR for Data Science by Hadley Wickham & Garrett Grolemund - A comprehensive guide to data science using R and the tidyverse.\n\nAdvanced R by Hadley Wickham - A deep dive into the R programming language and its advanced features.\nThe Art of R Programming by Norman Matloff - A thorough introduction to R programming for beginners and experienced programmers alike.\nR in Action by Robert Kabacoff - A practical guide to R with numerous examples and case studies.\nData Science with R by Garrett Grolemund & Hadley Wickham - A hands-on guide to data science using R.\nPractical Statistics for Data Scientists by Peter Bruce & Andrew Bruce - A practical guide to statistical methods for data science using R.\n\n\n\n\nList of R packages containers‚Ä¶\n\nCRAN - The Comprehensive R Archive Network, the primary repository for R packages.\nBioconductor - A repository for bioinformatics and computational biology packages in R.\nRopensci - A collection of R packages for open science and reproducible research.\n\n\n\n\nLinks to tutorials‚Ä¶\n\nRStudio Education - A variety of tutorials and resources for learning R and RStudio.\nSwirl - An interactive learning platform that teaches R programming and data science directly in the R console.\nDataCamp - Online courses and tutorials for learning R and data science.\nCoursera R Courses - A variety of R programming courses available on Coursera.\nYouTube Channels - Numerous channels and videos dedicated to R programming and data science.\n\n\n\n\nUseful references‚Ä¶\n\nRStudio Cheatsheets - Handy reference sheets for various R packages and functions.\nCRAN Task Views - Curated lists of R packages for specific topics.\nTidyverse - A collection of R packages designed for data science."
  },
  {
    "objectID": "content/rhacks/posts/09-03012029/index.html",
    "href": "content/rhacks/posts/09-03012029/index.html",
    "title": "What R Is Best For (And Why It Still Matters)",
    "section": "",
    "text": "The question is not:\nThe better question is:"
  },
  {
    "objectID": "content/rhacks/posts/09-03012029/index.html#vectorised-data-manipulation",
    "href": "content/rhacks/posts/09-03012029/index.html#vectorised-data-manipulation",
    "title": "What R Is Best For (And Why It Still Matters)",
    "section": "1Ô∏è‚É£ Vectorised Data Manipulation",
    "text": "1Ô∏è‚É£ Vectorised Data Manipulation\nIn R, most operations are naturally vectorised.\n\nmean(df$col)\ndf$col &gt; 100\ndf$new_col &lt;- df$col * 2\n\nNo explicit loops required.\nThis makes analytical code:\n\nconcise\nexpressive\nreadable\n\nR was built for data first ‚Äî not as a general-purpose programming language."
  },
  {
    "objectID": "content/rhacks/posts/09-03012029/index.html#statistical-modeling-out-of-the-box",
    "href": "content/rhacks/posts/09-03012029/index.html#statistical-modeling-out-of-the-box",
    "title": "What R Is Best For (And Why It Still Matters)",
    "section": "2Ô∏è‚É£ Statistical Modeling (Out of the Box)",
    "text": "2Ô∏è‚É£ Statistical Modeling (Out of the Box)\nR was designed for statistics.\nModeling syntax is natural:\n\nmodel &lt;- lm(y ~ x1 + x2, data = df)\nsummary(model)\n\nCompare this to lower-level implementations elsewhere.\nR‚Äôs formula interface is still one of the most elegant modeling abstractions available."
  },
  {
    "objectID": "content/rhacks/posts/09-03012029/index.html#reproducible-reporting",
    "href": "content/rhacks/posts/09-03012029/index.html#reproducible-reporting",
    "title": "What R Is Best For (And Why It Still Matters)",
    "section": "3Ô∏è‚É£ Reproducible Reporting",
    "text": "3Ô∏è‚É£ Reproducible Reporting\nR integrates seamlessly with:\n\nQuarto\nR Markdown\nparameterised reports\ndynamic documents\n\nYou can move from:\n\n\n\n\n\n\nTip\n\n\n\ndata ‚Üí model ‚Üí visual ‚Üí report\n\n\nin one environment.\nFew ecosystems make this as smooth."
  },
  {
    "objectID": "content/rhacks/posts/09-03012029/index.html#exploratory-data-analysis",
    "href": "content/rhacks/posts/09-03012029/index.html#exploratory-data-analysis",
    "title": "What R Is Best For (And Why It Still Matters)",
    "section": "4Ô∏è‚É£ Exploratory Data Analysis",
    "text": "4Ô∏è‚É£ Exploratory Data Analysis\nR encourages rapid iteration:\n\ndf |&gt;\n  group_by(group) |&gt;\n  summarise(mean_value = mean(value))\n\nThe tidyverse philosophy makes analytical transformations readable and composable.\nFor exploratory statistics, this is powerful."
  },
  {
    "objectID": "content/rhacks/posts/09-03012029/index.html#publication-ready-visualisation",
    "href": "content/rhacks/posts/09-03012029/index.html#publication-ready-visualisation",
    "title": "What R Is Best For (And Why It Still Matters)",
    "section": "5Ô∏è‚É£ Publication-Ready Visualisation",
    "text": "5Ô∏è‚É£ Publication-Ready Visualisation\nggplot2 remains one of the most coherent grammar-based plotting systems.\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  theme_minimal()\n\nLayered, declarative, expressive."
  },
  {
    "objectID": "content/rhacks/posts/09-03012029/index.html#where-r-is-not-always-best",
    "href": "content/rhacks/posts/09-03012029/index.html#where-r-is-not-always-best",
    "title": "What R Is Best For (And Why It Still Matters)",
    "section": "6Ô∏è‚É£ Where R Is Not Always Best",
    "text": "6Ô∏è‚É£ Where R Is Not Always Best\nR may not be the best choice for:\n\nlarge-scale production APIs\nfull-stack applications\nsystem-level programming\n\nAnd that‚Äôs fine.\nTools should be chosen by purpose, not ideology."
  },
  {
    "objectID": "content/rhacks/posts/09-03012029/index.html#the-practical-conclusion",
    "href": "content/rhacks/posts/09-03012029/index.html#the-practical-conclusion",
    "title": "What R Is Best For (And Why It Still Matters)",
    "section": "The Practical Conclusion",
    "text": "The Practical Conclusion\nIf your work is:\n\nstatistical\nanalytical\nexploratory\nreport-driven\nreproducible\n\nR is still one of the most efficient tools available.\nNot because it replaces everything.\nBut because it was designed for analysis.\n\n\n\n\n\n\nNote\n\n\n\nIn Short\n\nR excels at statistics and modeling\nR is expressive for analytical workflows\nR integrates naturally with reporting\nR remains extremely strong for EDA and visualisation\nChoose tools based on task, not trend\n\n\n\nR does not need to compete with everything.\nIt just needs to do what it does best.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to stay up to date with the latest events and posts from the Rome R Users Group:\nüëâ https://www.meetup.com/rome-r-users-group/"
  },
  {
    "objectID": "content/rhacks/posts/03-01132026/index.html",
    "href": "content/rhacks/posts/03-01132026/index.html",
    "title": "Compare multiple dataframes before binding them together",
    "section": "",
    "text": "Create your function\n\nThis hack is based on my Cyclistic analysis on Kaggle (see Chapter 6.3):\nüîó https://www.kaggle.com/code/lcolon/cyclistic-2023-google-da-capstone-project-r\nWhen working with multiple datasets a common question is:\n\nAre these dataframes really compatible before I bind them together?\n\nPeople who work with data typically rely on a mix of approaches to answer this:\nmanual checks (names(), glimpse()), warnings raised by bind_rows(), or dedicated helpers such as compare_df_cols() from the janitor package ‚Äî which is a solid and widely used solution for comparing column structures.\nHowever, in practice, it is often useful to have a single, lightweight helper that provides a compact and readable overview across many dataframes at once, without additional dependencies or verbose output.\nThis R Hack introduces a small custom helper function designed with that goal in mind, that you can easily reuse and adapt to your own workflows.\nIt performs an omnicomprehensive structural comparison before merging, checking column count, column names, and column classes across all pairs of dataframes:\n\n#   comparison   same_ncol same_names same_class\n#   &lt;chr&gt;        &lt;lgl&gt;     &lt;lgl&gt;      &lt;lgl&gt;     \n# 1 df_a vs df_b TRUE      TRUE       TRUE      \n# 2 df_a vs df_c TRUE      TRUE       FALSE     \n# 3 df_a vs df_d TRUE      FALSE      FALSE     \n# 4 df_b vs df_c TRUE      TRUE       FALSE     \n# 5 df_b vs df_d TRUE      FALSE      FALSE     \n# 6 df_c vs df_d TRUE      FALSE      FALSE"
  },
  {
    "objectID": "content/rhacks/posts/03-01132026/index.html#section",
    "href": "content/rhacks/posts/03-01132026/index.html#section",
    "title": "Compare multiple dataframes before binding them together",
    "section": "",
    "text": "Create your function\n\nThis hack is based on my Cyclistic analysis on Kaggle (see Chapter 6.3):\nüîó https://www.kaggle.com/code/lcolon/cyclistic-2023-google-da-capstone-project-r\nWhen working with multiple datasets a common question is:\n\nAre these dataframes really compatible before I bind them together?\n\nPeople who work with data typically rely on a mix of approaches to answer this:\nmanual checks (names(), glimpse()), warnings raised by bind_rows(), or dedicated helpers such as compare_df_cols() from the janitor package ‚Äî which is a solid and widely used solution for comparing column structures.\nHowever, in practice, it is often useful to have a single, lightweight helper that provides a compact and readable overview across many dataframes at once, without additional dependencies or verbose output.\nThis R Hack introduces a small custom helper function designed with that goal in mind, that you can easily reuse and adapt to your own workflows.\nIt performs an omnicomprehensive structural comparison before merging, checking column count, column names, and column classes across all pairs of dataframes:\n\n#   comparison   same_ncol same_names same_class\n#   &lt;chr&gt;        &lt;lgl&gt;     &lt;lgl&gt;      &lt;lgl&gt;     \n# 1 df_a vs df_b TRUE      TRUE       TRUE      \n# 2 df_a vs df_c TRUE      TRUE       FALSE     \n# 3 df_a vs df_d TRUE      FALSE      FALSE     \n# 4 df_b vs df_c TRUE      TRUE       FALSE     \n# 5 df_b vs df_d TRUE      FALSE      FALSE     \n# 6 df_c vs df_d TRUE      FALSE      FALSE"
  },
  {
    "objectID": "content/rhacks/posts/03-01132026/index.html#step-0-create-example-dataframes-to-compare",
    "href": "content/rhacks/posts/03-01132026/index.html#step-0-create-example-dataframes-to-compare",
    "title": "Compare multiple dataframes before binding them together",
    "section": "Step 0 ‚Äì Create example dataframes to compare",
    "text": "Step 0 ‚Äì Create example dataframes to compare\nTo make the idea concrete, let‚Äôs create four small dataframes. Some of them match perfectly, others don‚Äôt:\n\ndf_a &lt;- data.frame(\n  id = 1:3,\n  value = c(10, 20, 30)\n)\n\ndf_b &lt;- data.frame(\n  id = 4:6,\n  value = c(40, 50, 60)\n)\n\n# Same columns, but different class for `value`\ndf_c &lt;- data.frame(\n  id = 7:9,\n  value = as.character(c(70, 80, 90))\n)\n\n# Different column name\ndf_d &lt;- data.frame(\n  id = 10:12,\n  amount = c(100, 200, 300)\n)"
  },
  {
    "objectID": "content/rhacks/posts/03-01132026/index.html#step-1-define-the-helper-function-to-compare-dataframes",
    "href": "content/rhacks/posts/03-01132026/index.html#step-1-define-the-helper-function-to-compare-dataframes",
    "title": "Compare multiple dataframes before binding them together",
    "section": "Step 1 ‚Äì Define the helper function to compare dataframes",
    "text": "Step 1 ‚Äì Define the helper function to compare dataframes\nThe function below takes a vector of dataframe names (as character strings) and compares each unique pair only once. It works in three simple steps:\n\nIt loads the dataframes from the environment using their names\nIt iterates over all unique dataframe combinations, ensuring that each pair is checked only once (for example, it compares df_a vs df_b, but not df_b vs df_a))\n\nFor each pair, it performs the following structural checks and stores the results in a tibble:\n\nwhether the number of columns matches\nwhether column names are identical\nwhether column classes are the same\n\n\n\nThe function then binds all tibbles together into a single summary table, making it easy to spot structural mismatches before combining the data.\n\nlibrary(tidyverse)\ncompare_dataframes &lt;- function(df_names) {\n\n  # Get DF names\n  df_list &lt;- lapply(df_names, get)\n\n  # Initialize an empty list\n  results &lt;- list()\n\n  # Selection of DF pairs to be compared\n  for (i in seq_len(length(df_list) - 1)) {\n    for (j in (i + 1):length(df_list)) {\n\n      results[[length(results) + 1]] &lt;- tibble(\n        comparison = paste(df_names[i], \"vs\", df_names[j]),\n\n        # check number of columns\n        same_ncol  = ncol(df_list[[i]]) == ncol(df_list[[j]]),\n\n        # check columns name\n        same_names = identical(names(df_list[[i]]), names(df_list[[j]])),\n\n        # check columns class\n        same_class = identical(\n          sapply(df_list[[i]], class),\n          sapply(df_list[[j]], class)\n        )\n      )\n    }\n  }\n\n  comparison_result &lt;- bind_rows(results)\n  return(comparison_result)\n}"
  },
  {
    "objectID": "content/rhacks/posts/03-01132026/index.html#step-2-apply-the-function",
    "href": "content/rhacks/posts/03-01132026/index.html#step-2-apply-the-function",
    "title": "Compare multiple dataframes before binding them together",
    "section": "Step 2 ‚Äì Apply the function",
    "text": "Step 2 ‚Äì Apply the function\nProvide the dataframe names as a character vector and invoke the function:\n\ndf_names &lt;- c(\"df_a\", \"df_b\", \"df_c\", \"df_d\")\n\ncomparison_result &lt;- compare_dataframes(df_names)\n\ncomparison_result\n\n# A tibble: 6 √ó 4\n  comparison   same_ncol same_names same_class\n  &lt;chr&gt;        &lt;lgl&gt;     &lt;lgl&gt;      &lt;lgl&gt;     \n1 df_a vs df_b TRUE      TRUE       TRUE      \n2 df_a vs df_c TRUE      TRUE       FALSE     \n3 df_a vs df_d TRUE      FALSE      FALSE     \n4 df_b vs df_c TRUE      TRUE       FALSE     \n5 df_b vs df_d TRUE      FALSE      FALSE     \n6 df_c vs df_d TRUE      FALSE      FALSE"
  },
  {
    "objectID": "content/rhacks/posts/03-01132026/index.html#step-3-interpret-the-results",
    "href": "content/rhacks/posts/03-01132026/index.html#step-3-interpret-the-results",
    "title": "Compare multiple dataframes before binding them together",
    "section": "Step 3 ‚Äì Interpret the results",
    "text": "Step 3 ‚Äì Interpret the results\nThe output table makes structural mismatches immediately visible:\nsame_ncol ‚Üí different number of columns\nsame_names ‚Üí different column names\nsame_class ‚Üí same columns, but different data types\nIn the example provided:\ndf_a vs df_b ‚Üí all TRUE (safe to bind)\ndf_a vs df_c ‚Üí same_class = FALSE\ndf_a vs df_d ‚Üí same_names = FALSE\nThis helps you catch issues before they turn into silent bugs."
  },
  {
    "objectID": "content/rhacks/posts/03-01132026/index.html#in-short",
    "href": "content/rhacks/posts/03-01132026/index.html#in-short",
    "title": "Compare multiple dataframes before binding them together",
    "section": "In short",
    "text": "In short\n\nThere is no single ‚Äúofficial‚Äù way to compare many dataframes at once\nHelpers like compare_df_cols() from janitor are excellent for column-level inspection\nThis custom helper function provides a clear, compact overview before stacking or merging data\nThink of it as a pre-flight checklist before combining your data\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you want to stay up to date with the latest events from the Rome R Users Group, click here:\nüëâ https://www.meetup.com/rome-r-users-group/\nAnd if you are curious, the full Kaggle notebook used for this tip is available here:\nüîó https://www.kaggle.com/code/lcolon/cyclistic-2023-google-da-capstone-project-r"
  },
  {
    "objectID": "content/rhacks/posts/02-12052025/index.html",
    "href": "content/rhacks/posts/02-12052025/index.html",
    "title": "R Hack ‚Äì Getting started with maps in R using GeoJSON and library sf",
    "section": "",
    "text": "This hack is based on my Cyclistic analysis on Kaggle (see Chapter 8.12.4):\nüîó https://www.kaggle.com/code/lcolon/cyclistic-2023-google-da-capstone-project-r\nWorking with maps in R is easier than it seems.\nIf you want to visualize points, regions, or spatial patterns, the combination of GeoJSON files and the sf package gives you a simple and modern workflow.\nIn this hack, we‚Äôll load a real city map (Chicago) directly from a URL, plot it with geom_sf(), and overlay simple point data."
  },
  {
    "objectID": "content/rhacks/posts/02-12052025/index.html#step-0-what-is-a-geojson-file",
    "href": "content/rhacks/posts/02-12052025/index.html#step-0-what-is-a-geojson-file",
    "title": "R Hack ‚Äì Getting started with maps in R using GeoJSON and library sf",
    "section": "Step 0 ‚Äì What is a GeoJSON file?",
    "text": "Step 0 ‚Äì What is a GeoJSON file?\nA GeoJSON file is a JSON text file that stores geographic shapes (points, lines, polygons), together with their attributes (names, boundaries, codes, etc.).\nIt is one of the most common formats for open geographic data because it is lightweight, human-readable, and web-friendly.\nYou can often find GeoJSON files on:\n\ncity open data portals (City of Chicago, NYC Open Data, etc.)\n\nnational and international platforms (e.g., data.gov)\n\nGitHub repositories\n\nmany public mapping resources"
  },
  {
    "objectID": "content/rhacks/posts/02-12052025/index.html#step-1-load-the-required-packages",
    "href": "content/rhacks/posts/02-12052025/index.html#step-1-load-the-required-packages",
    "title": "R Hack ‚Äì Getting started with maps in R using GeoJSON and library sf",
    "section": "Step 1 ‚Äì Load the required packages",
    "text": "Step 1 ‚Äì Load the required packages\n\nlibrary(sf)       # for spatial data (simple features)\nlibrary(ggplot2)  # for plotting"
  },
  {
    "objectID": "content/rhacks/posts/02-12052025/index.html#step-2-read-a-geojson-map-with-sfst_read",
    "href": "content/rhacks/posts/02-12052025/index.html#step-2-read-a-geojson-map-with-sfst_read",
    "title": "R Hack ‚Äì Getting started with maps in R using GeoJSON and library sf",
    "section": "Step 2 ‚Äì Read a GeoJSON map with sf::st_read()\n",
    "text": "Step 2 ‚Äì Read a GeoJSON map with sf::st_read()\n\nIn this hack, we use a GeoJSON of Chicago community areas, provided by the City of Chicago Open Data Portal.\n‚ö†Ô∏è Before using any online map or dataset, always remember to check the terms of use of the source.\nYou can read a GeoJSON directly from a URL, without downloading anything manually:\n\nmap_of_chicago &lt;- sf::st_read(\n  \"https://data.cityofchicago.org/resource/y6yq-dbs2.geojson\",\n  quiet = TRUE\n  )\n\nhead(map_of_chicago)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.74141 ymin: 41.80189 xmax: -87.60641 ymax: 41.93272\nGeodetic CRS:  WGS 84\n           pri_neigh          sec_neigh    shape_area    shape_len\n1    Grand Boulevard        BRONZEVILLE 48492503.1554 28196.837157\n2       Printers Row       PRINTERS ROW 2162137.97139  6864.247156\n3      United Center      UNITED CENTER 32520512.7053 23101.363745\n4 Sheffield & DePaul SHEFFIELD & DEPAUL 10482592.2987 13227.049745\n5      Humboldt Park      HUMBOLDT PARK 125010425.593 46126.751351\n6      Garfield Park      GARFIELD PARK 89976069.5947  44460.91922\n                        geometry\n1 MULTIPOLYGON (((-87.60671 4...\n2 MULTIPOLYGON (((-87.62761 4...\n3 MULTIPOLYGON (((-87.66707 4...\n4 MULTIPOLYGON (((-87.65833 4...\n5 MULTIPOLYGON (((-87.7406 41...\n6 MULTIPOLYGON (((-87.6954 41...\n\n\n\nWhat you get:\n\n\nmap_of_chicago is an sf object -essentially a data frame with a special geometry column where each row is a polygon that represents one community area"
  },
  {
    "objectID": "content/rhacks/posts/02-12052025/index.html#step-3-plot-a-basic-map-with-geom_sf",
    "href": "content/rhacks/posts/02-12052025/index.html#step-3-plot-a-basic-map-with-geom_sf",
    "title": "R Hack ‚Äì Getting started with maps in R using GeoJSON and library sf",
    "section": "Step 3 ‚Äì Plot a basic map with geom_sf()\n",
    "text": "Step 3 ‚Äì Plot a basic map with geom_sf()\n\nOnce the GeoJSON file has been loaded into an sf object, you can visualize it with geom_sf().\ngeom_sf() is the ggplot2 layer designed specifically for spatial data: it knows how to read the geometry column of an sf object and automatically draw points, lines, or polygons depending on what the data contains. In our case, it will draw the polygons of Chicago‚Äôs community areas.\n\nggplot() +\n  geom_sf(data = map_of_chicago, \n          fill = \"#f1f0f0\", color = \"grey50\") +\n  labs(title = \"Chicago community areas\",\n       subtitle = \"GeoJSON map loaded with sf::st_read()\") +\n  theme_void()"
  },
  {
    "objectID": "content/rhacks/posts/02-12052025/index.html#to-recap",
    "href": "content/rhacks/posts/02-12052025/index.html#to-recap",
    "title": "R Hack ‚Äì Getting started with maps in R using GeoJSON and library sf",
    "section": "To Recap",
    "text": "To Recap\n\nGeoJSON = JSON file with geometry + attributes\nLoad it directly from a URL with sf::st_read()\nPlot it with geom_sf()\nAdd your own points using regular ggplot layers\nThis workflow is simple, beginner-friendly, and requires no GIS tools\n\n\nThanks for reading!\n\n\n\n\n\n\nTip\n\n\n\nIf you want to stay up to date with the latest events from the Rome R Users Group, click here:\nüëâ https://www.meetup.com/rome-r-users-group/\nAnd if you are curious, the full Kaggle notebook used for this tip is available here:\nüîó https://www.kaggle.com/code/lcolon/cyclistic-2023-google-da-capstone-project-r"
  },
  {
    "objectID": "content/rhacks/posts/08-02222026/index.html",
    "href": "content/rhacks/posts/08-02222026/index.html",
    "title": "CSV vs Parquet: A Better Default for Analytics",
    "section": "",
    "text": "If you are still saving analytical datasets as .csv by default, you are likely paying a hidden tax:\n:::{.callout-note callout-appearance: simple}\nCSV is universal. But universal does not mean optimal.\n:::\nThis R-Hack explains why Parquet should often be your default for analytical workflows."
  },
  {
    "objectID": "content/rhacks/posts/08-02222026/index.html#what-is-parquet",
    "href": "content/rhacks/posts/08-02222026/index.html#what-is-parquet",
    "title": "CSV vs Parquet: A Better Default for Analytics",
    "section": "What Is Parquet?",
    "text": "What Is Parquet?\nParquet is a columnar, binary storage format designed for analytics.\nUnlike CSV:\n\nIt stores column types explicitly\nIt compresses data efficiently\nIt reads columns independently\n\nIn R, Parquet support is provided by the arrow package.\n\n# Example R code chunk\n# Load necessary libraries\nlibrary(arrow)\nlibrary(dplyr)\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  id = 1:5,\n  value = c(10.5, 20.3, 30.8, 40.2, 50.1)\n)\n\n# Write the data frame to a Parquet file\nwrite_parquet(data, \"sample_data.parquet\")\n\n# Read the Parquet file back into R\ndata_read &lt;- read_parquet(\"sample_data.parquet\")\n\n# Print the data\nprint(data_read)\n\n# A tibble: 5 √ó 2\n     id value\n  &lt;int&gt; &lt;dbl&gt;\n1     1  10.5\n2     2  20.3\n3     3  30.8\n4     4  40.2\n5     5  50.1"
  },
  {
    "objectID": "content/rhacks/posts/08-02222026/index.html#step-1-simulate-a-realistic-dataset",
    "href": "content/rhacks/posts/08-02222026/index.html#step-1-simulate-a-realistic-dataset",
    "title": "CSV vs Parquet: A Better Default for Analytics",
    "section": "Step 1 ‚Äî Simulate a Realistic Dataset",
    "text": "Step 1 ‚Äî Simulate a Realistic Dataset\n\nset.seed(123)\n\ndf &lt;- data.frame(\n  id = 1:200000,\n  category = sample(LETTERS[1:5], 200000, replace = TRUE),\n  value = rnorm(200000),\n  flag = sample(c(TRUE, FALSE), 200000, replace = TRUE)\n)\n\nThis mimics a moderately sized analytical dataset."
  },
  {
    "objectID": "content/rhacks/posts/08-02222026/index.html#step-2-save-and-read-as-csv",
    "href": "content/rhacks/posts/08-02222026/index.html#step-2-save-and-read-as-csv",
    "title": "CSV vs Parquet: A Better Default for Analytics",
    "section": "Step 2 ‚Äî Save and Read as CSV",
    "text": "Step 2 ‚Äî Save and Read as CSV\n\nwrite.csv(df, \"data.csv\", row.names = FALSE)\n\nsystem.time({\n  df_csv &lt;- read.csv(\"data.csv\")\n})\n\n   user  system elapsed \n  0.264   0.012   0.286 \n\n\nNow check file size:\n\nfile.info(\"data.csv\")$size\n\n[1] 6819327\n\n\nCSV:\n\nstores everything as text\nrequires parsing every time\nguesses types on read"
  },
  {
    "objectID": "content/rhacks/posts/08-02222026/index.html#step-3-save-and-read-as-parquet",
    "href": "content/rhacks/posts/08-02222026/index.html#step-3-save-and-read-as-parquet",
    "title": "CSV vs Parquet: A Better Default for Analytics",
    "section": "Step 3 ‚Äî Save and Read as Parquet",
    "text": "Step 3 ‚Äî Save and Read as Parquet\n\nwrite_parquet(df, \"data.parquet\")\n\nsystem.time({\n  df_parquet &lt;- read_parquet(\"data.parquet\")\n})\n\n   user  system elapsed \n  0.007   0.001   0.007 \n\n\nCheck file size:\n\nfile.info(\"data.parquet\")$size\n\n[1] 3231687\n\n\nYou will typically observe:\n\nfaster read time\nsmaller file size\npreserved data types\n\n\nWhy This Matters in Practice?\n\n1Ô∏è‚É£ Speed\nParquet reads are faster because:\n\nit stores columns independently\nit avoids text parsing\nit uses compression effectively\n\nThis becomes significant as datasets grow.\n2Ô∏è‚É£ File Size\nParquet compresses automatically. In many real-world cases parquet files are 30‚Äì70% smaller than CSV equivalents:\n\nLess storage\nFaster transfer\nCleaner repositories\n3Ô∏è‚É£ Type Safety\nCSV does not store types.\nEvery read operation reinterprets:\n\nlogical values\nfactors\ndates\nnumeric columns\n\nParquet preserves them.\nThis reduces silent coercion bugs."
  },
  {
    "objectID": "content/rhacks/posts/08-02222026/index.html#bonus-select-columns-without-loading-everything",
    "href": "content/rhacks/posts/08-02222026/index.html#bonus-select-columns-without-loading-everything",
    "title": "CSV vs Parquet: A Better Default for Analytics",
    "section": "Bonus ‚Äî Select Columns Without Loading Everything",
    "text": "Bonus ‚Äî Select Columns Without Loading Everything\nWith arrow, you can load only what you need:\n\nread_parquet(\"data.parquet\", col_select = c(\"id\", \"value\"))\n\n# A tibble: 200,000 √ó 2\n      id   value\n   &lt;int&gt;   &lt;dbl&gt;\n 1     1  0.945 \n 2     2 -0.284 \n 3     3  0.395 \n 4     4  1.32  \n 5     5  0.872 \n 6     6  0.0354\n 7     7 -0.427 \n 8     8 -0.975 \n 9     9  0.0965\n10    10  0.808 \n# ‚Ñπ 199,990 more rows\n\n\nThis is especially powerful for:\n\nlarge analytical datasets\nremote storage\nmodular pipelines\n\nWhen CSV Is Still Appropriate\nCSV is fine when:\n\nexporting for non-technical users\nsharing small files\ngenerating human-readable outputs\n\n\nBut for analytical workflows?\n\nParquet is usually superior."
  },
  {
    "objectID": "content/rhacks/posts/08-02222026/index.html#workflow-recommendation",
    "href": "content/rhacks/posts/08-02222026/index.html#workflow-recommendation",
    "title": "CSV vs Parquet: A Better Default for Analytics",
    "section": "Workflow Recommendation",
    "text": "Workflow Recommendation\nFor reproducible projects:\n\nRaw data ‚Üí store as Parquet\nIntermediate processed data ‚Üí store as Parquet\nFinal exports ‚Üí optionally CSV\n\nThink of Parquet as your working format, and CSV as your exchange format.\n\n\n\n\n\n\nNote\n\n\n\nIn Short\n\nCSV is universal but inefficient\nParquet is faster, smaller, and type-safe\nSwitching requires only {arrow}\nMake Parquet your default for analysis projects\n\n\n\nModern workflows deserve modern storage.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to stay up to date with the latest events and posts from the Rome R Users Group:\nüëâ &lt;https://www.meetup.com/rome-r-users-group/"
  },
  {
    "objectID": "content/rhacks/posts/01-11282025/index.html",
    "href": "content/rhacks/posts/01-11282025/index.html",
    "title": "How to Add Images to ggplot Axes Using ggtext::element_markdown()\n",
    "section": "",
    "text": "This tip is based on a full Kaggle notebook, where this technique is applied in a complete Exploratory Data Analysis (EDA): üîó https://www.kaggle.com/code/lcolon/football-2023-2024-eda-and-pa-with-logos\n\nHave you ever needed to show logos, icons, or flags directly on the axes of your ggplot charts?\n\nYou can do this using the element_markdown() function from the {ggtext} package!\nelement_markdown() is a function that tells ggplot to treat labels as HTML instead of plain text. It must be used in the theme() section of your ggplot code.\nThe chart below shows an example where league logos appear directly on the x-axis. It displays the total number of goals scored in the major European football leagues during the 2023/2024 season:\n\n\n\nEuropean football leagues 2023/2024"
  },
  {
    "objectID": "content/rhacks/posts/01-11282025/index.html#section",
    "href": "content/rhacks/posts/01-11282025/index.html#section",
    "title": "How to Add Images to ggplot Axes Using ggtext::element_markdown()\n",
    "section": "",
    "text": "This tip is based on a full Kaggle notebook, where this technique is applied in a complete Exploratory Data Analysis (EDA): üîó https://www.kaggle.com/code/lcolon/football-2023-2024-eda-and-pa-with-logos\n\nHave you ever needed to show logos, icons, or flags directly on the axes of your ggplot charts?\n\nYou can do this using the element_markdown() function from the {ggtext} package!\nelement_markdown() is a function that tells ggplot to treat labels as HTML instead of plain text. It must be used in the theme() section of your ggplot code.\nThe chart below shows an example where league logos appear directly on the x-axis. It displays the total number of goals scored in the major European football leagues during the 2023/2024 season:\n\n\n\nEuropean football leagues 2023/2024"
  },
  {
    "objectID": "content/rhacks/posts/01-11282025/index.html#exploratory-data-analysis-workflow",
    "href": "content/rhacks/posts/01-11282025/index.html#exploratory-data-analysis-workflow",
    "title": "How to Add Images to ggplot Axes Using ggtext::element_markdown()\n",
    "section": "Exploratory Data Analysis Workflow",
    "text": "Exploratory Data Analysis Workflow\nHere is a short, practical guide showing how to recreate this plot.\nStep 0: Start from a basic dataframe\nIn most real situations you will already have a dataframe to work with. Here is the one used in this example:\n\ndf &lt;- data.frame(\n  league = c(\"Premier League\", \"La Liga\", \n             \"Serie A\", \"Bundesliga\", \"Ligue 1\"),\n  goals  = c(1197, 977, 968, 960, 804))\n\ndf\n\n          league goals\n1 Premier League  1197\n2        La Liga   977\n3        Serie A   968\n4     Bundesliga   960\n5        Ligue 1   804\n\n\nStep 1: Choose the images you want to display (PNG recommended)\nFirst decide which images you want to show on your ggplot axis. In this example I am using online image URLs (including links from Wikimedia), but you can use any source that provides a direct image URL, such as files hosted on GitHub repos or any public static hosting service. Just make sure the image format is supported ‚Äî typically PNG or JPG/JPEG.\n\n# Define image URLs as variables\npremier_url &lt;- \"https://upload.wikimedia.org/wikipedia/it/6/6d/Premier_League_Logo_2016.png\"\nlaliga_url &lt;- \"https://upload.wikimedia.org/wikipedia/it/9/9b/LFP_La_Liga.png\"\nseriea_url &lt;- \"https://www.fifplay.com/img/public/serie-a-logo.png\"\nbundes_url &lt;- \"https://www.fifplay.com/img/public/bundesliga-logo.png\"\nligue1_url &lt;- \"https://upload.wikimedia.org/wikipedia/commons/a/a0/Ligue_1_2024_Logo.png\"\n\nStep 2: Add a column with HTML  tags\nIn order to display images on the axis, each image URL must be wrapped inside an HTML  tag. This simply means that the URL cannot appear alone ‚Äî it must be placed inside the structure:\n`&lt;img src= + image_url + width='45'/&gt;`\nPlease note that in this example I use width=‚Äò45‚Äô just as a reference size, but you can choose any value depending on how big you want the image to appear in your plot.\nLet‚Äôs add the column to the df:\n\ndf$logo &lt;- c(\n  paste0(\"&lt;img src='\", premier_url, \"' width='45'/&gt;\"), \n  paste0(\"&lt;img src='\", laliga_url, \"' width='45'/&gt;\"), \n  paste0(\"&lt;img src='\", seriea_url, \"' width='45'/&gt;\"), \n  paste0(\"&lt;img src='\", bundes_url, \"' width='45'/&gt;\"), \n  paste0(\"&lt;img src='\", ligue1_url, \"' width='45'/&gt;\")\n  )\n\nAt this point, the dataframe should look like this:\n\n\n\n\n\n\n\n\nleague\ngoals\nlogo\n\n\n\nPremier League\n1197\n&lt;img src='https://upload.wikimedia.org/wikipedia/it/6/6d/Premier_League_Logo_2016.png' width='45'/&gt;\n\n\nLa Liga\n977\n&lt;img src='https://upload.wikimedia.org/wikipedia/it/9/9b/LFP_La_Liga.png' width='45'/&gt;\n\n\nSerie A\n968\n&lt;img src='https://www.fifplay.com/img/public/serie-a-logo.png' width='45'/&gt;\n\n\nBundesliga\n960\n&lt;img src='https://www.fifplay.com/img/public/bundesliga-logo.png' width='45'/&gt;\n\n\nLigue 1\n804\n&lt;img src='https://upload.wikimedia.org/wikipedia/commons/a/a0/Ligue_1_2024_Logo.png' width='45'/&gt;\n\n\n\n\nStep 3: Plot your data!\nNow that the dataframe contains a column of HTML  tags, we can use it directly on the x-axis. The key part is here:\n\ntheme( axis.text.x = element_markdown() )\n\nelement_markdown() function is what allows ggplot to interpret the HTML in your axis labels. Without it, ggplot would show the literal string &lt;img src='...'&gt; as plain text. With it, the &lt;img&gt; tags are rendered as actual images.\nHere is the full plotting code:\n\nlibrary(ggplot2)\nlibrary(ggtext)\n\n\noptions(repr.plot.width = 6, \n        repr.plot.height = 7, \n        warn = -1)\n\n\nggplot(df, \n       aes(x = reorder(logo, -goals), y = goals)) + \n  geom_col(fill = \"forestgreen\", width = .8) +\n  geom_text(aes(label = goals), vjust = -0.5, size = 4.5) + \n  labs(title = \"\\nFootball stats 2023/2024\",\n       subtitle = \"Total Goals scored in each League\\n\" ) + \n  theme_minimal(14) +\n  # HTML rendering happens here!!\n  theme(axis.text.x = ggtext::element_markdown(), \n         axis.text.y = element_blank(), \n         panel.grid = element_blank(), \n         axis.title = element_blank(), \n         plot.title = element_text(face = \"bold\", \n                                   colour = \"blue\", hjust = 0.05), \n         plot.subtitle= element_text(hjust = 0.05)\n         )\n\nWith element_markdown() applied to axis.text.x, ggplot correctly displays the league logos instead of plain text labels, as seen in the barchart above.\nTo Recap\n\nPrepare your dataframe (existing or new)\nChoose your images (PNG recommended) and store their URLs\nWrap each URL inside an HTML tag with a defined width\nAdd this HTML column to your dataframe\nMap it to the ggplot axis\nEnable HTML rendering in plot theme using:\n\n\naxis.text.x = ggtext::element_markdown()\n\nThanks for reading!\n\n\n\n\n\n\nTip\n\n\n\nIf you want to stay up to date with the latest events from the Rome R Users Group, click here:\nüëâhttps://www.meetup.com/rome-r-users-group/\nAnd if you are curious, the full Kaggle notebook used for this tip is available here: üîóhttps://www.kaggle.com/code/lcolon/football-2023-2024-eda-and-pa-with-logos"
  },
  {
    "objectID": "content/events/posts/01052025.html",
    "href": "content/events/posts/01052025.html",
    "title": "Rome in Person Meetup",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/events/posts/01102025_inperson.html",
    "href": "content/events/posts/01102025_inperson.html",
    "title": "Rome in Person Meetup",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/events/index.html",
    "href": "content/events/index.html",
    "title": "Events at Rome R Users Group",
    "section": "",
    "text": "Events are the heartbeat of our community. They bring us together to learn, collaborate, and connect. Whether it‚Äôs a workshop, a meetup, or a project showcase, each event is an opportunity to share knowledge, build skills, and grow the R programming community in Rome and beyond.\nOur in-person focused group events are designed to foster deeper conversations and hands-on collaboration in smaller, interactive settings. Unlike large meetups, these sessions create space for participants to dive into specific topics, exchange ideas, and work through challenges side by side. By bringing people together in the same room, we create a supportive environment where learning feels personal, networking happens naturally, and everyone has a chance to contribute."
  },
  {
    "objectID": "content/events/index.html#upcoming-events",
    "href": "content/events/index.html#upcoming-events",
    "title": "Events at Rome R Users Group",
    "section": "Upcoming Events",
    "text": "Upcoming Events\n\nFor More Information about Next Events/Workshops/Tutorials Please Check: https://www.meetup.com/rome-r-users-group/\n\n\n\n\n\nMark Griffin"
  },
  {
    "objectID": "content/events/index.html#past-events",
    "href": "content/events/index.html#past-events",
    "title": "Events at Rome R Users Group",
    "section": "Past Events",
    "text": "Past Events"
  },
  {
    "objectID": "about/sponsors.html",
    "href": "about/sponsors.html",
    "title": "Sponsor",
    "section": "",
    "text": "We are thrilled to announce our latest partnership with R Consortium, a pivotal player in the world of data science and programming. R Consortium is a non-profit organization committed to advancing the R language and fostering innovation within the R community."
  },
  {
    "objectID": "about/sponsors.html#introducing-our-new-sponsor-r-consortium",
    "href": "about/sponsors.html#introducing-our-new-sponsor-r-consortium",
    "title": "Sponsor",
    "section": "",
    "text": "We are thrilled to announce our latest partnership with R Consortium, a pivotal player in the world of data science and programming. R Consortium is a non-profit organization committed to advancing the R language and fostering innovation within the R community."
  },
  {
    "objectID": "about/sponsors.html#why-r-consortium",
    "href": "about/sponsors.html#why-r-consortium",
    "title": "Sponsor",
    "section": "Why R Consortium?",
    "text": "Why R Consortium?\nR Consortium‚Äôs mission aligns perfectly with our goals at R-Ladies Rome. They provide essential support to projects and initiatives that enhance R‚Äôs capabilities and accessibility. By partnering with R Consortium, we‚Äôre strengthening our commitment to empowering data enthusiasts, promoting R programming, and fostering a vibrant community."
  },
  {
    "objectID": "about/sponsors.html#what-this-means-for-you",
    "href": "about/sponsors.html#what-this-means-for-you",
    "title": "Sponsor",
    "section": "What This Means for You",
    "text": "What This Means for You\nThis partnership brings numerous benefits to our community. We‚Äôll have access to valuable resources, expertise, and opportunities that will further enrich our events and activities. Together, we‚Äôll continue to grow, learn, and share knowledge within the R programming ecosystem.\nWe‚Äôre grateful for the support of R Consortium and look forward to the exciting opportunities this collaboration will bring. Stay tuned for upcoming events and initiatives that will showcase the incredible possibilities unlocked by this partnership.\nLet‚Äôs embark on this journey together, embracing the power of R programming and data science with R Consortium by our side. Here‚Äôs to a bright future of learning, innovation, and community-building!\n#RConsortium #RProgramming #DataScience #CommunityBuilding"
  }
]